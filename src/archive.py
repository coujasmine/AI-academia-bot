"""
Archive manager for date-based report storage and README index updates.

Saves each run's output to archives/YYYY-MM-DD/ with both a Markdown report
(human-readable on GitHub) and a JSON dump (machine-readable backup).
Automatically maintains a history table in the project README.
"""

import json
import logging
import re
import shutil
from datetime import datetime, timedelta
from pathlib import Path

from config.settings import BASE_DIR, ARCHIVE_RETENTION_DAYS

logger = logging.getLogger(__name__)

ARCHIVES_DIR = BASE_DIR / "archives"
README_PATH = BASE_DIR / "README.md"


class ArchiveManager:
    """Manages date-based archiving of paper data and report files."""

    def __init__(self, root_dir: Path | None = None):
        self.root_dir = root_dir or ARCHIVES_DIR
        self.today = datetime.now().strftime("%Y-%m-%d")
        self.current_dir = self.root_dir / self.today

    def save(self, papers: list[dict], report_path: Path | None = None) -> Path:
        """Archive papers to a dated folder with JSON + Markdown.

        Args:
            papers: List of normalized paper dicts.
            report_path: Existing report file to copy into the archive.
                         If None, generates a minimal Markdown report.

        Returns:
            Path to the archive directory.
        """
        if not papers:
            logger.info("No papers to archive")
            return self.current_dir

        self.current_dir.mkdir(parents=True, exist_ok=True)

        self._save_json(papers)
        self._save_markdown(papers, report_path)
        self._cleanup_old_archives()
        self._update_readme_index()

        logger.info("Archive saved to: %s", self.current_dir)
        return self.current_dir

    def _save_json(self, papers: list[dict]):
        """Save raw paper data as JSON for programmatic access."""
        path = self.current_dir / "data.json"
        with open(path, "w", encoding="utf-8") as f:
            json.dump(papers, f, indent=2, ensure_ascii=False)
        logger.info("JSON data saved: %s (%d papers)", path, len(papers))

    def _save_markdown(self, papers: list[dict], report_path: Path | None = None):
        """Save a Markdown report into the archive folder.

        If a report was already generated by report.py, copy its content.
        Otherwise, generate a minimal report inline.
        """
        dest = self.current_dir / "report.md"

        if report_path and report_path.exists():
            content = report_path.read_text(encoding="utf-8")
            dest.write_text(content, encoding="utf-8")
            logger.info("Markdown report archived from: %s", report_path)
        else:
            self._generate_minimal_report(papers, dest)

    def _generate_minimal_report(self, papers: list[dict], dest: Path):
        """Generate a minimal Markdown report when no existing report is available."""
        lines = [
            f"# Paper Report: {self.today}\n",
            f"> **Total Papers:** {len(papers)} | **Source:** FT50 & UTD24\n",
            "---\n",
        ]

        for i, p in enumerate(papers, 1):
            title = p.get("title", "Untitled")
            link = p.get("doi") or p.get("url", "")
            lines.append(f"### {i}. {title}\n")
            lines.append(f"- **Journal:** {p.get('journal_abbr', 'N/A')}")
            authors = p.get("authors", [])
            if authors:
                lines.append(f"- **Authors:** {', '.join(authors[:5])}")
            lines.append(f"- **Date:** {p.get('publication_date', 'N/A')}")
            if link:
                lines.append(f"- **Link:** [{link}]({link})")
            abstract = p.get("abstract", "")
            if abstract:
                if len(abstract) > 300:
                    abstract = abstract[:300] + "..."
                lines.append(f"> {abstract}")
            lines.append("")

        dest.write_text("\n".join(lines), encoding="utf-8")
        logger.info("Minimal Markdown report generated: %s", dest)

    def _cleanup_old_archives(self):
        """Remove archive folders older than ARCHIVE_RETENTION_DAYS (~2 months)."""
        if not self.root_dir.exists():
            return

        cutoff = datetime.now() - timedelta(days=ARCHIVE_RETENTION_DAYS)
        cutoff_str = cutoff.strftime("%Y-%m-%d")
        removed = 0

        for d in list(self.root_dir.iterdir()):
            if not d.is_dir() or not re.match(r"\d{4}-\d{2}-\d{2}", d.name):
                continue
            if d.name < cutoff_str:
                shutil.rmtree(d)
                removed += 1
                logger.info("Removed old archive: %s", d.name)

        if removed:
            logger.info("Cleaned up %d archive(s) older than %d days", removed, ARCHIVE_RETENTION_DAYS)
   
    def _update_readme_index(self):
        """Update the history reports table in README.md between HISTORY markers.

        IMPORTANT: The README history table is rebuilt solely from existing archives/* folders.
        It does NOT read/append prior README content.
        """
        if not README_PATH.exists():
            logger.warning("README.md not found, skipping index update")
            return

        content = README_PATH.read_text(encoding="utf-8")

        # ✅ Match your README markers (HISTORY_START/END), not ARCHIVE_START/END
        pattern = r"(<!-- HISTORY_START -->)(.*?)(<!-- HISTORY_END -->)"
        if not re.search(pattern, content, flags=re.DOTALL):
            logger.warning(
                "History markers (<!-- HISTORY_START/END -->) not found in README. "
                "Index not updated."
            )
            return

        if not self.root_dir.exists():
            return

        # ✅ Only include dates that have BOTH report.md and data.json (real, complete archives)
        dates = sorted(
            [
                d.name
                for d in self.root_dir.iterdir()
                if d.is_dir()
                and re.fullmatch(r"\d{4}-\d{2}-\d{2}", d.name)
                and (d / "report.md").exists()
                and (d / "data.json").exists()
            ],
            reverse=True,
        )

        # ✅ Build ONLY the table (do NOT include "## History Reports" heading here)
        table_lines = [
            "| Date | Report | Data |",
            "|---|---|---|",
        ]
        for d in dates:
            table_lines.append(
                f"| {d} | [Report](archives/{d}/report.md) | [JSON](archives/{d}/data.json) |"
            )

        new_block = "\n".join(table_lines) + "\n"

        updated = re.sub(
            pattern,
            f"<!-- HISTORY_START -->\n{new_block}<!-- HISTORY_END -->",
            content,
            flags=re.DOTALL,
        )

        README_PATH.write_text(updated, encoding="utf-8")
        logger.info("README history updated with %d archive entries", len(dates))
